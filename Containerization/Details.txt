Application development methodologies (for large-scale applications be designed, built and maintained):
2014 - We must adopt #microservices (SOA) to solve all problems with monoliths (a large single packaged application)
	Disadvantages:
	Large numbers of microservices (with different technology stacks) are difficult to orchestrate and secure.
	Understanding, managing and testing dependencies is difficult.
	Message flow increases with the number of microservices and hampers performance.
	Rich and complex business logic in enterprise applications will be difficult to maintain in microservice architectures.
	Once requirements change, do you want a microservice for one new feature or re-write and add to an existing service is a difficult decision.
	
2016 - We must adopt #docker to solve all problems with microservices
	Docker guarantees that application microservices will run in their own environments that are completely separate from the operating system.
	A load of technological supports are necessary for Docker implementation including orchestration, container management, app stack, data screenshots, networking of containers, and so on.

2018 - We must adopt #kubernetes to solve all problems with docker
	Docker now comes with a special mode – swarm mode – that you can use to manage clusters of containers. Docker Swarm lets you use the Docker CLI to run swarm commands, so you can easily initialize groups of containers and add and remove containers from those groups. Besides Docker Swarm, there are several other container orchestration managers you could consider as well: Kubernetes, a containers cluster manager. You can run Kubernetes on your own servers or in the cloud.
	
Container, containerization and container orchestration:
A container is the software package that comprises an application’s code, configurations, and dependencies which presents operational efficiency and productivity.
Containerization executes the applications portable by virtualizing at the operating-system level, producing separated, encapsulated systems which are kernel based. Containerized apps can be left in anywhere and run without dependencies or needing a whole VM, excluding dependencies. But container orchestration is needed when there are multiple containers.
Container orchestration is the method that can usually deploy multiple containers to execute an application by automation. Platforms like Kubernetes and Docker Swarm are the container management and container orchestration engines that allow users to control container deployment and automate updates, health monitoring, and failover systems.
Note: https://apachebooster.com/blog1/2019/03/kubernetes-vs-docker-swarm-comparison/

************https://www.javacodegeeks.com/2018/12/docker-jvm.html   *************

Why should I put Java in a container anyway?
This is a good question to ask. Wasn’t Java built with the “Write once, run anywhere” slogan? Even though true, this statement only encompasses the Java binaries. Your bytecode (i.e.Jar file) will run fine on every possible version of the JVM. However what about database drivers? File system access? Networking? Available entropy? Third party application you rely on? All these things will vary across Operating Systems. Often your application will require a fine balance between your Java binaries and any of those third party dependencies. If you have ever supported Java applications installed by your clients, you will know what I mean.

First came the VMs
Before containers, the universal solution was to use Virtual Machines. Create a new blank virtual computer with the operating system of your choice, install all of your third-party dependencies, copy your Java binaries, take a snapshot and finally ship it. With a VM you are certain that what you ship runs exactly the way it is supposed to run and also consistently each time. There is no room for environment configuration issues. They also provide strong encapsulation. If you run your application in the cloud, each virtual machines will be strongly isolated. There is very limited space for corruption between VMs running on the same hardware.
But, there is always a but, they are heavy. If you found a bug in your application and had to change one line of code, you must recompile your Jar file, reinstall the VM and ship the whole thing. One line of code turns into several GB files to be uploaded to the cloud or downloaded at your clients. Operating System files are heavy, probably much heavier than your Java binaries, and you have to ship them every time even though they don’t really change.

Containers to the rescue
As explained above, VM have their own copy of the OS whereas containers are made to be smaller and contain just what you want to ship:
With a container, the OS(to be exact it is the Kernel that is being shared, you can choose to build images from different distributions like Ubuntu, Debian, Alpine etc…) is provided by the engine (e.g. Docker) and you don’t have to ship with your application.
With Docker, you ship images which are built in layers. The instructions for building an image is put into a Dockerfile.
Every time a container is created in Docker, it has direct access to the host machine and its resources. Docker containers are lightweight. When you start a container, it loads within a second or less since it doesn’t have to boot its own operating system. This is different from virtual machines, which need to boot an OS when you start them, which takes several minutes.
Docker is used to run software packages called containers. Containers are isolated from each other and bundle their own tools, libraries and configuration files; they can communicate with each other through well-defined channels.

From Deployment standpoint: Containers offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. This decoupling allows container-based applications to be deployed easily and consistently, regardless of whether the target environment is a private data center, the public cloud, or even a developer’s personal laptop.
From an operations standpoint: Apart from portability containers also give more granular control over resources giving your infrastructure improved efficiency which can result in better utilization of your compute resources.

The Docker Desktop installation includes 
1.Docker Engine(dockerd - Docker Engine, a portable, lightweight runtime and packaging tool), 
2.Docker CLI client(docker), 
3.Docker Compose (Compose is a tool for defining and running multi-container Docker applications => https://docs.docker.com/compose/), 
4.Docker Machine (Docker Machine is a tool that lets you install Docker Engine on virtual hosts, and manage the hosts with docker-machine commands => https://docs.docker.com/machine/overview/), and 
5.Kitematic (A legacy GUI, bundled with Docker Toolbox, for managing Docker containers. Kitematic integrates with Docker Machine to provision a VirtualBox VM and install the Docker Engine locally on your machine.Once installed, the Kitematic GUI launches and from the home screen you are presented with curated images that you can run instantly).

Docker Engine is a client-server application with these major components:
	A server which is a type of long-running program called a daemon process (the dockerd command).
	A REST API which specifies interfaces that programs can use to talk to the daemon and instruct it what to do.
	A command line interface (CLI) client (the docker command).
		The Docker daemon -> The Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage Docker services.
		The Docker client -> The Docker client (docker) is the primary way that many Docker users interact with Docker. When you use commands such as docker run, the client sends these commands to dockerd, which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon.
		Docker registries -> A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker is configured to look for images on Docker Hub by default. You can even run your own private registry. If you use Docker Datacenter (DDC), it includes Docker Trusted Registry (DTR). When you use the docker pull or docker run commands, the required images are pulled from your configured registry. When you use the docker push command, your image is pushed to your configured registry.
		Docker objects -> When you use Docker, you are creating and using images, containers, networks, volumes, plugins, and other objects.
			IMAGES => An image is a read-only template with instructions for creating a Docker container. Often, an image is based on another image, with some additional customization. For example, you may build an image which is based on the ubuntu image, but installs the Apache web server and your application, as well as the configuration details needed to make your application run. You might create your own images or you might only use those created by others and published in a registry. To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it. Each instruction in a Dockerfile creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies.
				official images found at => https://hub.docker.com/search?image_filter=official&type=image
			CONTAINERS => A container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI. You can connect a container to one or more networks, attach storage to it, or even create a new image based on its current state. By default, a container is relatively well isolated from other containers and its host machine. You can control how isolated a container’s network, storage, or other underlying subsystems are from other containers or from the host machine. A container is defined by its image as well as any configuration options you provide to it when you create or start it. When a container is removed, any changes to its state that are not stored in persistent storage disappear.
			SERVICES => Services allow you to scale containers across multiple Docker daemons, which all work together as a swarm with multiple managers and workers. Each member of a swarm is a Docker daemon, and the daemons all communicate using the Docker API. A service allows you to define the desired state, such as the number of replicas of the service that must be available at any given time. By default, the service is load-balanced across all worker nodes. To the consumer, the Docker service appears to be a single application. Docker Engine supports swarm mode in Docker 1.12 and higher.
		Docker underlying technologies
			Namespaces => Docker uses a technology called namespaces to provide the isolated workspace called the container. When you run a container, Docker creates a set of namespaces for that container. These namespaces provide a layer of isolation. Each aspect of a container runs in a separate namespace and its access is limited to that namespace. Docker Engine uses namespaces such as the following on Linux:
				The pid namespace: Process isolation (PID: Process ID).
				The net namespace: Managing network interfaces (NET: Networking).
				The ipc namespace: Managing access to IPC resources (IPC: InterProcess Communication).
				The mnt namespace: Managing filesystem mount points (MNT: Mount).
				The uts namespace: Isolating kernel and version identifiers. (UTS: Unix Timesharing System).
		Control groups => Docker Engine on Linux also relies on another technology called control groups (cgroups). A cgroup limits an application to a specific set of resources. Control groups allow Docker Engine to share available hardware resources to containers and optionally enforce limits and constraints. For example, you can limit the memory available to a specific container.
		Union file systems => Union file systems, or UnionFS, are file systems that operate by creating layers, making them very lightweight and fast. Docker Engine uses UnionFS to provide the building blocks for containers. Docker Engine can use multiple UnionFS variants, including AUFS, btrfs, vfs, and DeviceMapper.
		Container format => Docker Engine combines the namespaces, control groups, and UnionFS into a wrapper called a container format. The default container format is libcontainer. In the future, Docker may support other container formats by integrating with technologies such as BSD Jails or Solaris Zones.

What can I use Docker for?
1.Fast, consistent delivery of your applications
2.Responsive deployment and scaling
3.Running more workloads on the same hardware

Common commands:
docker run hello-world
	To generate this message, Docker took the following steps:
	 1. The Docker client contacted the Docker daemon.
	 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.
		(amd64)
	 3. The Docker daemon created a new container from that image which runs the
		executable that produces the output you are currently reading.
	 4. The Docker daemon streamed that output to the Docker client, which sent it
		to your terminal.
docker-machine ls
docker-machine rm my-docker-machine
C:\Users\<your-user>\.docker => This directory stores some Docker program configuration and state, such as information about created machines and certificates. You usually don’t need to remove this directory.
docker build -f /path/to/a/Dockerfile .
docker build -t shykes/myapp:1.0.2 -t shykes/myapp:latest .

Dockerfile commands (https://docs.docker.com/develop/develop-images/dockerfile_best-practices/):
FROM ubuntu:18.04
COPY . /app
RUN make /app
CMD python /app/app.py
--- 
FROM alpine:latest
RUN apk --update add openjdk8-jre
COPY docker-hello-world-example.jar app.jar
ENTRYPOINT ["java", "-Djava.security.egd=file:/dev/./urandom", "-jar", "app.jar"]
---
mkdir myproject && cd myproject
echo "hello" > hello
echo -e "FROM busybox\nCOPY /hello /\nRUN cat /hello" > Dockerfile
docker build -t helloapp:v1 .
mkdir -p dockerfiles context
mv Dockerfile dockerfiles && mv hello context
docker build --no-cache -t helloapp:v2 -f dockerfiles/Dockerfile context




Conceptually a Dockerfile could be something like:
Start with a blank Ubuntu distribution
Install Java
Install dependency A
Install dependency B
Copy jar file
(actual Dockerfile example available below)

Each instruction in the Dockerfile creates an immutable layer. This is clever and a great optimisation. If you keep the instruction that copies your Java binaries at the end (and you should), you only have to change the last layer when you make a code change. That means if you are shipping change for the end-users, you only need to upload the last layer, the previous unchanged layers are cached; end users will only need to download the changed layers from the bottom of the image. With Docker, a change in one line of code means only a few MB upload/download (While if this was the VM the changes would be in GB instead of MB).
=============== 
One of the key features of Docker is the ability to limit a container’s memory and CPU usage. This is one of the main reason why it’s economically interesting to run many Docker containers in the Cloud. Orchestration solutions like Kubernetes (k8s) will try to efficiently “pack” containers on multiple nodes. Here “pack” refers to memory and CPU (see how k8s plays Tetris for you). If you give sensible boundaries for memory and CPU to your Docker containers, K8s will be able to efficiently arrange them on multiple nodes.
=============== 

The architecture of containers is the other major benefit. There's now a standard way to divide applications into distributed objects or containers. Breaking applications up this way offers the ability to place them on different physical and virtual machines, in the cloud or not. This flexibility offers more advantages around workload management and provides the ability to easily make fault-tolerant systems.
As the container world continues to emerge, it's becoming difficult to build container applications without these management layers.

Docker, the most popular container standard, is an open-source project that provides a way to automate the deployment of applications inside software containers. Docker really started the container movement. However, it's not the only game in town. Companies such as CoreOS have their own container standard called Rocket, and many standards and products are being built around these technologies.

Containers have a few basic features and advantages, including the ability to:
1.Reduce complexity through container abstractions. Containers don't require dependencies on the application infrastructure. Thus, you don't need a complex native interface to deal with platform services.
2.Leverage automation to maximize portability. Automation replaced manual scripting. These days, it's much more difficult to guarantee portability when using automation.
3.Provide better security and governance, external to the containers. Security and governance services are platform-specific, not application-specific. Placing security and governance services outside of the container significantly reduces complexity.
4.Provide enhanced distributed computing capabilities. This is due to the fact that an application can be divided into many domains, all residing within containers. The portability aspect of containers means they can execute on a number of different cloud platforms. This allows engineers to pick and choose the platforms that they run on, based upon cost and performance efficiencies.
5.Provide automation services that leverage policy-based optimization. There needs to be an automation layer that can locate the best platform to execute on, and auto-migrate to that platform. At the same time, it must automatically deal with needed configuration changes.

How to scale container-based applications
Most who look to make containers scale take one of two basic approaches. The first approach is to create a custom system to manage the containers. This means a one-off system that you build to automatically launch new container instances as needed to handle an increasing processing load. But remember that if you build it, you own it. As with many DIY approaches, the maintenance will become labor- and cost-intensive.

The second approach is to leverage one of the container orchestration, scheduling, and clustering technologies that will provide the basic mechanisms to enable scalability. This is normally the better of the two options.

There are a few choices out there for the second approach:

First, Google's Kubernetes is an open-source container cluster manager, much like Docker Swarm (discussed below). Kubernetes can schedule any number of container replicas across a group of node instances. This container replication and distribution trick is typically enough to make most large container-based applications scale as needed. This is pretty much the same approach to scaling containers that the other tools take.

Second, Cloudify provides a Docker orchestration tool that overlaps with Docker Compose and Docker Swarm. Its YAML-based blueprints let developers describe complex topologies, including the infrastructure, middleware tier, and app layers. It's more orchestration-oriented, and thus should be considered when looking at orchestration and automation tools when clustering isn't needed.

Finally, the newest tool, Docker Swarm provides clustering, scheduling, and integration capabilities. This tool enables developers to build and ship multi-container/multi-host distributed applications that include the necessary scaling and management for container-based systems. Obviously, Swarm is designed to compete with Kubernetes, which has a larger market share. Consider both tools when there's a need to massively scale containers. I would suggest a proof of concept with each technology, using real-world workloads.

Best practices continue to emerge around scaling containers, including:

Devote time to the architecture of your container-based applications. Most scaling issues are traced back to poor designs, not poor technology.
Always do a proof of concept to determine the real scaling capabilities of the solutions you're considering. Use automated testing tools to simulate the workloads and massive amounts of data for testing.
Consider your own requirements. What works for other large companies may not be right for your container-based applications.
Don't forget about security and governance. They have to scale as well.
I suspect that scaling containers will be a bit tricky until more is understood about how containers behave at scale. However, with a good understanding of the proper use of containers and the right technology, you'll be scalable right out of the gate.

Containerization in nutshell
A container is an isolated execution environment where one or many processes can run in isolation. The action of creating containers and running your application as a process inside it; is known as containerization. 

A process inside the container has isolated environment. That includes network interface (to obtain IP addresses), process ids (PIDs), mount points etc. Linux Kernel out of the box provides some of the features like Namespaces and Control Groups to make this happen.

Namespaces are the features of Linux Kernel that partition kernel resources like Network Interface (net), Mount Points (mnt), Process Ids (PIDs) etc. Hence we can create sets of processes having same resource identifiers, for example, IP Addresses because they share different namespaces. Control Groups are also namespaced and they control how much system resources like CPU and Memory is allocated to sets of processes.

In general (and not specifically related to Linux), a container is nothing but a set of processes we just talked about. A container has unique namespace and all processes running inside it will have their share of resources allocated by container’s control group. Any process inside the container will not be able to see or interface with the resources allocated to other other containers. All containers shares same kernel (of the host operating system) and when a container needs different kernel, then virtualization has to be provided.


Start a container running a redis server:
Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache and message broker. It supports data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs, geospatial indexes with radius queries and streams. 
docker run --name my-redis -d redis

